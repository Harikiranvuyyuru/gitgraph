{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os \n",
      "import sys\n",
      "\n",
      "os.environ['PYSPARK_SUBMIT_ARGS']='--master spark://ip-172-31-0-189:7077 --executor-memory 10240m --driver-memory 10240m'\n",
      "\n",
      "spark_home = os.environ.get('SPARK_HOME', None)\n",
      "if not spark_home:\n",
      "  raise ValueError('SPARK_HOME environment variable is not set')\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
      "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
      "execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Welcome to\n",
        "      ____              __\n",
        "     / __/__  ___ _____/ /__\n",
        "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
        "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.1\n",
        "      /_/\n",
        "\n",
        "Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)\n",
        "SparkContext available as sc, HiveContext available as sqlContext.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark import SparkContext\n",
      "from pyspark.sql import SQLContext"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.stop()\n",
      "sc = SparkContext(\"spark://ip-172-31-0-189:7077\", \"following\")\n",
      "sqlContext = SQLContext(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df11 = sqlContext.jsonFile(\"hdfs://ec2-52-8-127-252.us-west-1.compute.amazonaws.com:9000/data_jan2015/2011-*.*\")\n",
      "# df12 = sqlContext.jsonFile(\"hdfs://ec2-52-8-127-252.us-west-1.compute.amazonaws.com:9000/data_jan2015/2012-*.*\")\n",
      "# df13 = sqlContext.jsonFile(\"hdfs://ec2-52-8-127-252.us-west-1.compute.amazonaws.com:9000/data_jan2015/2013-*.*\")\n",
      "# df14 = sqlContext.jsonFile(\"hdfs://ec2-52-8-127-252.us-west-1.compute.amazonaws.com:9000/data_jan2015/2014-*.*\")\n",
      "df15 = sqlContext.jsonFile(\"hdfs://ec2-52-8-127-252.us-west-1.compute.amazonaws.com:9000/data2015/*.*\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_corrupt_record actor                created_at           id         org                  payload              public repo                 type            \n",
        "null            [https://secure.g... 2011-02-12T00:00:00Z 1127195475 [https://secure.g... [null,bess,887fa2... true   [355634,projectbl... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:06Z 1127195541 null                 [started,sosedoff... true   [1357116,ezmobius... WatchEvent      \n",
        "null            [https://secure.g... 2011-02-12T00:00:10Z 1127195568 null                 [null,francoisreg... true   [null,/,https://a... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:10Z 1127195569 null                 [null,Selenium2,3... true   [1253617,Selenium... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:17Z 1127195580 [https://secure.g... [null,joshthecode... true   [681382,appcelera... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:19Z 1127195629 null                 [null,urzuae,7c93... true   [1246284,urzuae/s... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:22Z 1127195641 null                 [null,arokem,f75a... true   [832431,arokem/ni... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:23Z 1127195642 null                 [null,jkbrooks,50... true   [1345302,jkbrooks... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:25Z 1127195649 null                 [null,networkx,83... true   [890377,networkx/... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:27Z 1127195668 null                 [null,spurious,43... true   [1281265,spurious... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:27Z 1127195669 null                 [null,qooxdoo,4d1... true   [548213,qooxdoo/q... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:29Z 1127195735 null                 [null,rawomega,13... true   [1306721,rawomega... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:29Z 1127195736 null                 [closed,gray-,f78... true   [1332060,gray-/ge... IssuesEvent     \n",
        "null            [https://secure.g... 2011-02-12T00:00:30Z 1127195743 null                 [started,dkujawsk... true   [1356006,joearms/... WatchEvent      \n",
        "null            [https://secure.g... 2011-02-12T00:00:31Z 1127195746 null                 [null,urzuae,7c93... true   [1243816,urzuae/s... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:32Z 1127195750 null                 [null,ardcore,1a1... true   [1356751,ardcore/... PushEvent       \n",
        "null            [https://secure.g... 2011-02-12T00:00:32Z 1127195751 null                 [started,matagus,... true   [1352324,jdriscol... WatchEvent      \n",
        "null            [https://secure.g... 2011-02-12T00:00:33Z 1127195757 null                 [closed,ardcore,1... true   [1356751,ardcore/... PullRequestEvent\n",
        "null            [https://secure.g... 2011-02-12T00:00:33Z 1127195797 null                 [null,mulander,ec... true   [null,/,https://a... FollowEvent     \n",
        "null            [https://secure.g... 2011-02-12T00:00:41Z 1127195802 [https://secure.g... [opened,hlindberg... true   [1251049,cloudsmi... IssuesEvent     \n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch = df11.filter(\"type='WatchEvent'\")\n",
      "df11_commit = df11.filter(\"type='CommitCommentEvent'\")\n",
      "df11_fork = df11.filter(\"type='ForkEvent'\")\n",
      "sqlContext.registerDataFrameAsTable(df11_watch, \"df11_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_commit, \"df11_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_fork, \"df11_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch_altered = sqlContext.sql(\"SELECT actor, created_at, repo, type FROM df11_watch_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch_altered_rdd = df11_watch_altered.map(lambda x: {\"actor\": x.actor.login, \"created_at\":x.created_at, \"type\":x.type, \"repo\":x.repo.name}).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch_altered_df = sqlContext.createDataFrame(df11_watch_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/spark/python/pyspark/sql/context.py:169: UserWarning: Using RDD of dict to inferSchema is deprecated,please use pyspark.sql.Row instead\n",
        "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated,\"\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch_altered_df.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actor          repo                 type      \n",
        "sosedoff       ezmobius/super-nginx WatchEvent\n",
        "dkujawski      joearms/SEBG         WatchEvent\n",
        "matagus        jdriscoll/django-... WatchEvent\n",
        "hail2u         wincent/Command-T    WatchEvent\n",
        "matagus        acdha/NativeImaging  WatchEvent\n",
        "christianrojas mislav/will_paginate WatchEvent\n",
        "graeson        facebook/php-sdk     WatchEvent\n",
        "graeson        cakephp/cakephp      WatchEvent\n",
        "matagus        pixelmatrix/mapkey   WatchEvent\n",
        "graeson        sebastianbergmann... WatchEvent\n",
        "graeson        phpbb/phpbb          WatchEvent\n",
        "ppl            pieter/gitx          WatchEvent\n",
        "graeson        openid/php-openid    WatchEvent\n",
        "rxgx           getify/LABjs         WatchEvent\n",
        "dkujawski      erlang/otp           WatchEvent\n",
        "martinpucala   opichals/osx-pkg-dmg WatchEvent\n",
        "mumoshu        scala/scala          WatchEvent\n",
        "Andrewmp1      blueimp/jQuery-Fi... WatchEvent\n",
        "elliottgoodwin intridea/authbuttons WatchEvent\n",
        "dkujawski      nitrogen/nitrogen    WatchEvent\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_watch_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df11_watch_table\")\n",
      "df11_commit_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df11_commit_table\")\n",
      "df11_fork_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df11_fork_table\")\n",
      "\n",
      "df11_watch_altered_rdd = df11_watch_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df11_commit_altered_rdd = df11_commit_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df11_fork_altered_rdd = df11_fork_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "\n",
      "df11_watch_altered_df = sqlContext.createDataFrame(df11_watch_altered_rdd)\n",
      "df11_commit_altered_df = sqlContext.createDataFrame(df11_commit_altered_rdd)\n",
      "df11_fork_altered_df = sqlContext.createDataFrame(df11_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlContext.registerDataFrameAsTable(df11_watch_altered_df, \"df11_watch_altered_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_commit_altered_df, \"df11_commit_altered_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_fork_altered_df, \"df11_fork_altered_table\")\n",
      "\n",
      "df11_altered_union = sqlContext.sql(\"SELECT * from df11_watch_altered_table UNION ALL SELECT * from df11_commit_altered_table UNION ALL SELECT * from df11_fork_altered_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df11_altered_union.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actor          repo                 type      \n",
        "sosedoff       ezmobius/super-nginx WatchEvent\n",
        "dkujawski      joearms/SEBG         WatchEvent\n",
        "matagus        jdriscoll/django-... WatchEvent\n",
        "hail2u         wincent/Command-T    WatchEvent\n",
        "matagus        acdha/NativeImaging  WatchEvent\n",
        "christianrojas mislav/will_paginate WatchEvent\n",
        "graeson        facebook/php-sdk     WatchEvent\n",
        "graeson        cakephp/cakephp      WatchEvent\n",
        "matagus        pixelmatrix/mapkey   WatchEvent\n",
        "graeson        sebastianbergmann... WatchEvent\n",
        "graeson        phpbb/phpbb          WatchEvent\n",
        "ppl            pieter/gitx          WatchEvent\n",
        "graeson        openid/php-openid    WatchEvent\n",
        "rxgx           getify/LABjs         WatchEvent\n",
        "dkujawski      erlang/otp           WatchEvent\n",
        "martinpucala   opichals/osx-pkg-dmg WatchEvent\n",
        "mumoshu        scala/scala          WatchEvent\n",
        "Andrewmp1      blueimp/jQuery-Fi... WatchEvent\n",
        "elliottgoodwin intridea/authbuttons WatchEvent\n",
        "dkujawski      nitrogen/nitrogen    WatchEvent\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# from sets import Set\n",
      "user_repo_map11 = df11_altered_union.map(lambda x: (x.actor, list([x.repo]))).groupByKey() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_repo11 = user_repo_map11.map(lambda x: {\"username\":x[0], \"repo\":[user for sublist in x[1] for user in sublist]}).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_repo11_df = sqlContext.createDataFrame(user_repo11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(user_repo11_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pyspark.sql.dataframe.DataFrame'>\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(user_repo11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "148188\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_repo11_df.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "repo                 username       \n",
        "ArrayBuffer(webte... linkyndy       \n",
        "ArrayBuffer(visio... spencermountain\n",
        "ArrayBuffer(k9mai... remram44       \n",
        "ArrayBuffer(jwand... in5ikt         \n",
        "ArrayBuffer(amazo... antonbabenko   \n",
        "ArrayBuffer(icsha... null           \n",
        "ArrayBuffer(wavde... larsericsson   \n",
        "ArrayBuffer(camer... kerryfalk      \n",
        "ArrayBuffer(route... PaulMarcel     \n",
        "ArrayBuffer(aloha... jacwright      \n",
        "ArrayBuffer(zendf... vchabot        \n",
        "ArrayBuffer(kfaus... medihack       \n",
        "ArrayBuffer(gabri... lovoror        \n",
        "ArrayBuffer(sprou... peiwang        \n",
        "ArrayBuffer(Pasca... sintaxasn      \n",
        "ArrayBuffer(jquer... isaias         \n",
        "ArrayBuffer(/, Sn... binaryone      \n",
        "ArrayBuffer(haxza... xiuide         \n",
        "ArrayBuffer(pauli... pvermaer       \n",
        "ArrayBuffer(Atmos... herder         \n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r11 = df11.first()\n",
      "print r11\n",
      "print \"\\n\"\n",
      "print type(r11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Row(_corrupt_record=None, actor=Row(avatar_url=u'https://secure.gravatar.com/avatar/18d2645401f42785041370096ca2e6b2?d=http://github.dev%2Fimages%2Fgravatars%2Fgravatar-user-420.png', gravatar_id=u'18d2645401f42785041370096ca2e6b2', id=490969, login=u'fourfingerwu', url=u'https://api.github.dev/users/fourfingerwu'), created_at=u'2011-02-12T10:00:07Z', id=u'1127718087', org=None, payload=Row(action=None, actor=None, actor_gravatar=None, after=None, before=None, comment_id=None, commit=None, desc=None, description=None, forkee=None, head=None, id=None, issue=None, issue_id=None, master_branch=None, member=None, name=u'nodeapp', number=None, object=u'branch', object_name=u'master', original=None, page_name=None, pull_request=None, push_id=None, ref=None, ref_type=None, repo=None, sha=None, shas=None, size=None, snippet=None, summary=None, target=None, title=None, url=None), public=True, repo=Row(id=1358184, name=u'fourfingerwu/nodeapp', url=u'https://api.github.dev/repos/fourfingerwu/nodeapp'), type=u'CreateEvent')\n",
        "\n",
        "\n",
        "<class 'pyspark.sql.types.Row'>\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(r11.actor)\n",
      "print r11.actor.login\n",
      "print r11.repo\n",
      "print r11.repo.name\n",
      "print r11.type\n",
      "print \"\\n\"\n",
      "print r11.payload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'pyspark.sql.types.Row'>\n",
        "fourfingerwu\n",
        "Row(id=1358184, name=u'fourfingerwu/nodeapp', url=u'https://api.github.dev/repos/fourfingerwu/nodeapp')\n",
        "fourfingerwu/nodeapp\n",
        "CreateEvent\n",
        "\n",
        "\n",
        "Row(action=None, actor=None, actor_gravatar=None, after=None, before=None, comment_id=None, commit=None, desc=None, description=None, forkee=None, head=None, id=None, issue=None, issue_id=None, master_branch=None, member=None, name=u'nodeapp', number=None, object=u'branch', object_name=u'master', original=None, page_name=None, pull_request=None, push_id=None, ref=None, ref_type=None, repo=None, sha=None, shas=None, size=None, snippet=None, summary=None, target=None, title=None, url=None)\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df12.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_corrupt_record actor                actor_attributes created_at           id         org                  payload              public repo                 repository type          url \n",
        "null            {\"gravatar_id\":\"b... null             2012-01-01T00:00:09Z 1508512236 null                 [null,null,null,n... true   [3055800,knowledg... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"7... null             2012-01-01T00:00:09Z 1508512237 null                 [null,null,null,n... true   [3081214,ozanturg... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"a... null             2012-01-01T00:00:12Z 1508512238 null                 [null,null,null,n... true   [3007212,eatnumbe... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"b... null             2012-01-01T00:00:12Z 1508512241 null                 [started,null,nul... true   [3079723,ronreite... null       WatchEvent    null\n",
        "null            {\"gravatar_id\":\"8... null             2012-01-01T00:00:14Z 1508512245 [https://secure.g... [null,null,null,n... true   [487828,DMDirc/DM... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"1... null             2012-01-01T00:00:15Z 1508512248 [https://secure.g... [null,null,null,n... true   [1012339,mozilla/... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"f... null             2012-01-01T00:00:16Z 1508512249 null                 [null,null,null,n... true   [3077770,breakhea... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"6... null             2012-01-01T00:00:18Z 1508512251 null                 [null,null,null,n... true   [3081069,adambyra... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"8... null             2012-01-01T00:00:19Z 1508512253 [https://secure.g... [null,null,null,n... true   [491743,DMDirc/Ut... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"e... null             2012-01-01T00:00:20Z 1508512254 null                 [null,null,null,n... true   [3021499,thedjpet... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"8... null             2012-01-01T00:00:22Z 1508512257 [https://secure.g... [null,null,null,n... true   [1954037,habari/s... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"9... null             2012-01-01T00:00:24Z 1508512258 null                 [null,null,null,n... true   [2906737,josebet1... null       CreateEvent   null\n",
        "null            {\"gravatar_id\":\"4... null             2012-01-01T00:00:27Z 1508512259 [https://secure.g... [null,null,null,n... true   [3074618,narvik-s... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"9... null             2012-01-01T00:00:30Z 1508512260 null                 [null,null,null,n... true   [2260424,hagleitn... null       DownloadEvent null\n",
        "null            {\"gravatar_id\":\"c... null             2012-01-01T00:00:31Z 1508512262 null                 [null,null,null,n... true   [1744616,andreaso... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"2... null             2012-01-01T00:00:31Z 1508512265 null                 [null,null,null,n... true   [3081229,shindows... null       CreateEvent   null\n",
        "null            {\"gravatar_id\":\"b... null             2012-01-01T00:00:32Z 1508512266 null                 [null,null,null,n... true   [3054454,isaacsan... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"e... null             2012-01-01T00:00:36Z 1508512267 [https://secure.g... [null,null,null,n... true   [1782396,sympy/pl... null       PushEvent     null\n",
        "null            {\"gravatar_id\":\"1... null             2012-01-01T00:00:53Z 1508512272 null                 [started,null,nul... true   [2929426,taitems/... null       WatchEvent    null\n",
        "null            {\"gravatar_id\":\"5... null             2012-01-01T00:00:55Z 1508512273 null                 [started,null,nul... true   [2939440,addyosma... null       WatchEvent    null\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df12_watch = df12.filter(\"type='WatchEvent'\")\n",
      "df12_commit = df12.filter(\"type='CommitCommentEvent'\")\n",
      "df12_fork = df12.filter(\"type='ForkEvent'\")\n",
      "sqlContext.registerDataFrameAsTable(df12_watch, \"df12_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df12_commit, \"df12_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df12_fork, \"df12_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "df12_watch_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df12_watch_table\")\n",
      "df12_commit_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df12_commit_table\")\n",
      "df12_fork_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df12_fork_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "df12_watch_altered_rdd = df12_watch_altered.map(lambda x: {\"actor\": json.loads(x.actor)['login'], \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df12_commit_altered_rdd = df12_commit_altered.map(lambda x: {\"actor\": json.loads(x.actor)['login'], \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df12_fork_altered_rdd = df12_fork_altered.map(lambda x: {\"actor\": json.loads(x.actor)['login'], \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "\n",
      "df12_watch_altered_df = sqlContext.createDataFrame(df12_watch_altered_rdd)\n",
      "df12_commit_altered_df = sqlContext.createDataFrame(df12_commit_altered_rdd)\n",
      "df12_fork_altered_df = sqlContext.createDataFrame(df12_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1662 in stage 121.0 failed 4 times, most recent failure: Lost task 1662.3 in stage 121.0 (TID 200931, ip-172-31-29-54.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-193-202613ee9b91>\", line 1, in <lambda>\n  File \"/usr/lib/python2.7/json/__init__.py\", line 338, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python2.7/json/decoder.py\", line 384, in raw_decode\n    raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-193-202613ee9b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf12_watch_altered_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf12_watch_altered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'login'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf12_commit_altered_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf12_commit_altered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'login'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf12_fork_altered_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf12_fork_altered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'login'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf12_watch_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf12_watch_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1662 in stage 121.0 failed 4 times, most recent failure: Lost task 1662.3 in stage 121.0 (TID 200931, ip-172-31-29-54.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-193-202613ee9b91>\", line 1, in <lambda>\n  File \"/usr/lib/python2.7/json/__init__.py\", line 338, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python2.7/json/decoder.py\", line 384, in raw_decode\n    raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r12 = df12.first()\n",
      "print r12\n",
      "print \"\\n\"\n",
      "print type(r12)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Row(_corrupt_record=None, actor=u'{\"gravatar_id\":\"b55e2cae26595d21039ad1bc05db5950\",\"url\":\"https://api.github.dev/users/kp-backup\",\"id\":1287779,\"login\":\"kp-backup\",\"avatar_url\":\"https://secure.gravatar.com/avatar/b55e2cae26595d21039ad1bc05db5950?d=http://github.dev%2Fimages%2Fgravatars%2Fgravatar-user-420.png\"}', actor_attributes=None, created_at=u'2012-01-01T00:00:09Z', id=u'1508512236', org=None, payload=Row(action=None, after=None, before=None, comment=None, comment_id=None, commit=None, commits=[Row(author=Row(email=u'roman.apostol+backup@gmail.com', name=u'Knowledge Point'), distinct=None, message=u'backup at Sun Jan  1 00:00:02 UTC 2012', sha=u'ad9010cbf0ecfd252c873ea7530342291f3e574b', url=u'https://api.github.com/repos/knowledge-point/tinypm-backup/commits/ad9010cbf0ecfd252c873ea7530342291f3e574b')], desc=None, description=None, download=None, forkee=None, gist=None, head=u'ad9010cbf0ecfd252c873ea7530342291f3e574b', id=None, issue=None, issue_id=None, legacy=None, master_branch=None, member=None, name=None, number=None, pages=None, pull_request=None, push_id=55756268, ref=u'refs/heads/master', ref_type=None, shas=None, size=1, target=None, url=None), public=u'true', repo=Row(id=3055800, name=u'knowledge-point/tinypm-backup', url=u'https://api.github.dev/repos/knowledge-point/tinypm-backup'), repository=None, type=u'PushEvent', url=None)\n",
        "\n",
        "\n",
        "<class 'pyspark.sql.types.Row'>\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "print json.loads(r12.actor)['login']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kp-backup\n"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "print type(json.loads(r12.actor))\n",
      "r12d = json.loads(r12.actor)\n",
      "print r12d['login']\n",
      "print r12.type\n",
      "print r12.repo\n",
      "print r12.repo.name\n",
      "print r12.created_at\n",
      "print r12.repository"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'dict'>\n",
        "kp-backup\n",
        "PushEvent\n",
        "Row(id=3055800, name=u'knowledge-point/tinypm-backup', url=u'https://api.github.dev/repos/knowledge-point/tinypm-backup')\n",
        "knowledge-point/tinypm-backup\n",
        "2012-01-01T00:00:09Z\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df13.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actor           actor_attributes     created_at           payload              public repository           type        url                 \n",
        "theopatt        [null,null,null,1... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-31T00:38... PushEvent   https://github.co...\n",
        "moinmoin        [null,null,null,e... 2013-01-01T00:00:... [opened,null,null... true   [2012-12-12T13:33... IssuesEvent https://github.co...\n",
        "msnrkjwr        [null,null,null,a... 2013-01-01T00:00:... [null,null,null,n... true   [2010-11-04T05:33... ForkEvent   https://github.co...\n",
        "xyk2            [xyk2maker.com,nu... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-04T09:27... PushEvent   https://github.co...\n",
        "mitchellh       [http://mitchellh... 2013-01-01T00:00:... [null,null,null,n... true   [2012-11-28T12:09... PushEvent   https://github.co...\n",
        "kelsonzhao      [null,hw,kelson.3... 2013-01-01T00:00:... [null,null,null,n... true   null                 FollowEvent https://github.co...\n",
        "qnikst          [null,Saint-Peter... 2013-01-01T00:00:... [null,null,null,n... true   [2011-02-03T12:41... PushEvent   https://github.co...\n",
        "thamizhchelvan  [null,null,null,b... 2013-01-01T00:00:... [null,null,null,n... true   [2013-01-01T00:00... CreateEvent https://github.co...\n",
        "multidude       [http://multidude... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-31T08:18... PushEvent   https://github.co...\n",
        "IanDBird        [null,null,null,d... 2013-01-01T00:00:... [null,null,null,n... true   [2011-06-16T09:19... PushEvent   https://github.co...\n",
        "Gringill        [null,null,dannys... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-31T23:54... CreateEvent https://github.co...\n",
        "pajtai          [http://netlumina... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-25T17:57... PushEvent   https://github.co...\n",
        "scottmuc        [http://scottmuc.... 2013-01-01T00:00:... [null,null,null,n... true   [2013-01-01T00:00... CreateEvent https://github.co...\n",
        "gxcsoccer       [null,Huawei,gxcs... 2013-01-01T00:00:... [null,null,null,n... true   [2013-01-01T00:00... CreateEvent https://github.co...\n",
        "towerhe         [http://hetao.im,... 2013-01-01T00:00:... [started,null,nul... true   [2012-11-25T07:01... WatchEvent  https://github.co...\n",
        "EclipseBundoran [http://www.eclip... 2013-01-01T00:01:... [null,null,null,n... true   [2012-01-18T09:12... PushEvent   https://github.co...\n",
        "orecchionebruno [null,null,null,e... 2013-01-01T00:01:... [opened,null,null... true   [2011-06-08T00:27... IssuesEvent https://github.co...\n",
        "skadyan         [null,,null,a0727... 2013-01-01T00:00:... [null,null,null,n... true   [2012-11-27T08:31... PushEvent   https://github.co...\n",
        "graphitemaster  [null,null,killfi... 2013-01-01T00:00:... [null,null,null,n... true   [2012-04-09T03:11... PushEvent   https://github.co...\n",
        "wrhansen        [null,Riders Disc... 2013-01-01T00:00:... [null,null,null,n... true   [2012-12-31T23:18... PushEvent   https://github.co...\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df13_watch = df13.filter(\"type='WatchEvent'\")\n",
      "df13_commit = df13.filter(\"type='CommitCommentEvent'\")\n",
      "df13_fork = df13.filter(\"type='ForkEvent'\")\n",
      "sqlContext.registerDataFrameAsTable(df11_watch, \"df13_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_commit, \"df13_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df11_fork, \"df13_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df13_watch_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df13_watch_table\")\n",
      "df13_commit_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df13_commit_table\")\n",
      "df13_fork_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df13_fork_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df13_watch_altered_rdd = df13_watch_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\n",
      "# df13_commit_altered_rdd = df13_commit_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\n",
      "# df13_fork_altered_rdd = df13_fork_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 123.0 failed 4 times, most recent failure: Lost task 17.3 in stage 123.0 (TID 201064, ip-172-31-13-239.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-202-f142bc6fcc85>\", line 1, in <lambda>\nAttributeError: 'Row' object has no attribute 'repository'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-202-f142bc6fcc85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf13_watch_altered_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf13_watch_altered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df13_commit_altered_rdd = df13_commit_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df13_fork_altered_rdd = df13_fork_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 17 in stage 123.0 failed 4 times, most recent failure: Lost task 17.3 in stage 123.0 (TID 201064, ip-172-31-13-239.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-202-f142bc6fcc85>\", line 1, in <lambda>\nAttributeError: 'Row' object has no attribute 'repository'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df13_watch_altered_df = sqlContext.createDataFrame(df13_watch_altered_rdd)\n",
      "df13_commit_altered_df = sqlContext.createDataFrame(df13_commit_altered_rdd)\n",
      "df13_fork_altered_df = sqlContext.createDataFrame(df13_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r13 = df13.first()\n",
      "print r13\n",
      "print \"\\n\"\n",
      "print type(r13)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Row(actor=u'taphu', actor_attributes=Row(blog=None, company=None, email=None, gravatar_id=u'3a898ae8f206cdab345dfd6c8495b16a', location=None, login=u'taphu', name=u'Rick Owens', type=u'User'), created_at=u'2013-01-01T11:00:37-08:00', payload=Row(action=None, comment=None, comment_id=None, commit=None, desc=None, description=None, head=None, id=None, issue=None, issue_id=None, master_branch=None, member=None, name=None, number=None, pages=None, pull_request=None, ref=None, ref_type=None, shas=None, size=None, target=None, url=None), public=True, repository=Row(created_at=u'2010-04-17T23:39:16-07:00', description=u'The Racket repository', fork=False, forks=85, has_downloads=True, has_issues=True, has_wiki=True, homepage=u'http://racket-lang.org/', id=615969, integrate_branch=None, language=u'Racket', master_branch=None, name=u'racket', open_issues=10, organization=None, owner=u'plt', private=False, pushed_at=u'2013-01-01T09:26:18-08:00', size=18788, stargazers=293, url=u'https://github.com/plt/racket', watchers=293), type=u'ForkEvent', url=u'https://github.com/taphu/racket')\n",
        "\n",
        "\n",
        "<class 'pyspark.sql.types.Row'>\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(r13.actor)\n",
      "print r13.actor_attributes.login\n",
      "print r13.repository.name\n",
      "print r13.repository.owner+\"/\"+r13.repository.name\n",
      "print r13.repository.language\n",
      "print r13.repository"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'unicode'>\n",
        "taphu\n",
        "racket\n",
        "plt/racket\n",
        "Racket\n",
        "Row(created_at=u'2010-04-17T23:39:16-07:00', description=u'The Racket repository', fork=False, forks=85, has_downloads=True, has_issues=True, has_wiki=True, homepage=u'http://racket-lang.org/', id=615969, integrate_branch=None, language=u'Racket', master_branch=None, name=u'racket', open_issues=10, organization=None, owner=u'plt', private=False, pushed_at=u'2013-01-01T09:26:18-08:00', size=18788, stargazers=293, url=u'https://github.com/plt/racket', watchers=293)\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_corrupt_record actor           actor_attributes     created_at           payload              public repository           type               url                 \n",
        "null            raghothams      [null,SAP Labs Pv... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            raghothams      [null,SAP Labs Pv... 2014-01-01T00:02:... [closed,null,null... true   [null,null,null,n... PullRequestEvent   https://github.co...\n",
        "null            terkel          [http://terkel.jp... 2014-01-01T00:02:... [opened,null,null... true   [null,null,null,n... IssuesEvent        https://github.co...\n",
        "null            yknx4           [null,null,null,d... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            myronmarston    [myronmars.to/n,M... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... DeleteEvent        https://github.com/ \n",
        "null            miyamo          [null,null,null,4... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... CreateEvent        https://github.co...\n",
        "null            myronmarston    [myronmars.to/n,M... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            myronmarston    [myronmars.to/n,M... 2014-01-01T00:01:... [closed,null,null... true   [null,null,null,n... PullRequestEvent   https://github.co...\n",
        "null            makopronto      [null,null,null,n... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            wp-plugins-user [http://www.plugi... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            derand          [http://www.deran... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            wp-plugins-user [http://www.plugi... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... CreateEvent        https://github.co...\n",
        "null            wp-plugins-user [http://www.plugi... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... CreateEvent        https://github.co...\n",
        "null            NSError         [pax-imperia.com,... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            tmsanrinsha     [http://sanrinsha... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            dasakigr        [null,null,null,9... 2014-01-01T00:02:... [started,null,nul... true   [null,null,null,n... WatchEvent         https://github.co...\n",
        "null            boozaa          [null,null,null,8... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            bors            [http://en.wikipe... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n",
        "null            bors            [http://en.wikipe... 2014-01-01T00:02:... [null,null,496492... true   [null,null,null,n... CommitCommentEvent https://github.co...\n",
        "null            Castaglia       [www.castaglia.or... 2014-01-01T00:02:... [null,null,null,n... true   [null,null,null,n... PushEvent          https://github.co...\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch = df14.filter(\"type='WatchEvent'\")\n",
      "df14_commit = df14.filter(\"type='CommitCommentEvent'\")\n",
      "df14_fork = df14.filter(\"type='ForkEvent'\")\n",
      "sqlContext.registerDataFrameAsTable(df14_watch, \"df14_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df14_commit, \"df14_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df14_fork, \"df14_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_watch_table\")\n",
      "df14_commit_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_commit_table\")\n",
      "df14_fork_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_fork_table\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def repo(x):\n",
      "    try:\n",
      "        reponame = x.repository.owner+\"/\"+x.repository.name\n",
      "        return reponame\n",
      "    except:\n",
      "        return \"\"\n",
      "        \n",
      "\n",
      "df14_watch_altered_rdd = df14_watch_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":repo(x)}).collect()\n",
      "df14_commit_altered_rdd = df14_commit_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":repo(x)}).collect()\n",
      "df14_fork_altered_rdd = df14_fork_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":repo(x)}).collect()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch_altered_df = sqlContext.createDataFrame(df14_watch_altered_rdd)\n",
      "df14_commit_altered_df = sqlContext.createDataFrame(df14_commit_altered_rdd)\n",
      "df14_fork_altered_df = sqlContext.createDataFrame(df14_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'df14_watch_altered_rdd' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-0d3e4698a05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf14_watch_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf14_watch_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf14_commit_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf14_commit_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf14_fork_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf14_fork_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'df14_watch_altered_rdd' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(df14_watch_altered_rdd)\n",
      "print df14_watch_altered_rdd[1:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n",
        "[{'repo': u'JacksonTian/fks', 'type': u'WatchEvent', 'actor': u'satlxq'}, {'repo': u'l4ka/pistachio', 'type': u'WatchEvent', 'actor': u'ya790206'}, {'repo': u'Semantic-Org/Semantic-UI', 'type': u'WatchEvent', 'actor': u'leplay'}, {'repo': u'JamieMason/ImageOptim-CLI', 'type': u'WatchEvent', 'actor': u'rrichards'}, {'repo': u'Zohaibaig/html5shiv', 'type': u'WatchEvent', 'actor': u'Zohaibaig'}, {'repo': u'alphagov/frontend', 'type': u'WatchEvent', 'actor': u'DQvsRA'}, {'repo': u'turingou/koa-guide', 'type': u'WatchEvent', 'actor': u'dontaskcece'}, {'repo': u'jackaudio/jack2', 'type': u'WatchEvent', 'actor': u'evojimmy'}, {'repo': u'benweet/stackedit', 'type': u'WatchEvent', 'actor': u'hick'}, {'repo': u'SEL-Columbia/bamboo', 'type': u'WatchEvent', 'actor': u'kingofhawks'}, {'repo': u'pypa/virtualenv', 'type': u'WatchEvent', 'actor': u'FreedomKnight'}, {'repo': u'designmodo/startup-demo', 'type': u'WatchEvent', 'actor': u'stupidbodo'}, {'repo': u'hyspace/typeahead.js-bootstrap3.less', 'type': u'WatchEvent', 'actor': u'noetix'}, {'repo': u'nosql/aggregations-2', 'type': u'WatchEvent', 'actor': u'oplichta'}, {'repo': u'Peaker/lamdu', 'type': u'WatchEvent', 'actor': u'AndrewBrinker'}, {'repo': u'chippydip/HearthLog', 'type': u'WatchEvent', 'actor': u'myrodin'}, {'repo': u'mutualmobile/lavaca', 'type': u'WatchEvent', 'actor': u'ruckfull'}, {'repo': u'bitcoin/bitcoin', 'type': u'WatchEvent', 'actor': u'igorsyl'}, {'repo': u'farhadi/html5sortable', 'type': u'WatchEvent', 'actor': u'dasakigr'}]\n"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "check = sqlContext.sql(\"SELECT actor, repository, type FROM df14_commit_table WHERE repository='NoneType'\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 236
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print check.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch_altered_df = sqlContext.createDataFrame(df15_watch_altered_rdd)\n",
      "df15_commit_altered_df = sqlContext.createDataFrame(df15_commit_altered_rdd)\n",
      "df15_fork_altered_df = sqlContext.createDataFrame(df15_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trial = sqlContext.sql(\"SELECT repository FROM df14_watch_table WHERE actor='raghothams'\")\n",
      "trial.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "repository          \n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n",
        "[null,null,null,n...\n"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_watch_table\")\n",
      "df14_commit_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_commit_table\")\n",
      "df14_fork_altered = sqlContext.sql(\"SELECT actor, repository, type FROM df14_fork_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o39.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'repository' given input columns _corrupt_record, id, actor, payload, repo, created_at, public, org, type;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:46)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:46)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:44)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:44)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:40)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1080)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:933)\n\tat sun.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-219-a2215a71d463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf14_watch_altered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT actor, repository, type FROM df14_watch_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf14_commit_altered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT actor, repository, type FROM df14_commit_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf14_fork_altered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT actor, repository, type FROM df14_fork_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \"\"\"\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'repository' given input columns _corrupt_record, id, actor, payload, repo, created_at, public, org, type;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:54)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:46)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:251)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:46)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:44)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:44)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:40)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1080)\n\tat org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)\n\tat org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:933)\n\tat sun.reflect.GeneratedMethodAccessor109.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch_altered_rdd = df14_watch_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\n",
      "# df14_commit_altered_rdd = df14_commit_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\n",
      "# df14_fork_altered_rdd = df14_fork_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 143.0 failed 4 times, most recent failure: Lost task 18.3 in stage 143.0 (TID 211205, ip-172-31-13-241.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-217-88bcda195b54>\", line 1, in <lambda>\nAttributeError: 'Row' object has no attribute 'repository'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-217-88bcda195b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf14_watch_altered_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf14_watch_altered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"actor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df14_commit_altered_rdd = df14_commit_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df14_fork_altered_rdd = df14_fork_altered.map(lambda x: {\"actor\": x.actor, \"type\":x.type, \"repo\":x.repository.owner+\"/\"+x.repository.name}).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 143.0 failed 4 times, most recent failure: Lost task 18.3 in stage 143.0 (TID 211205, ip-172-31-13-241.us-west-1.compute.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/usr/local/spark/python/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/serializers.py\", line 236, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-217-88bcda195b54>\", line 1, in <lambda>\nAttributeError: 'Row' object has no attribute 'repository'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df14_watch_altered_df = sqlContext.createDataFrame(df14_watch_altered_rdd)\n",
      "df14_commit_altered_df = sqlContext.createDataFrame(df14_commit_altered_rdd)\n",
      "df14_fork_altered_df = sqlContext.createDataFrame(df14_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r14 = df14.first()\n",
      "print r14\n",
      "print \"\\n\"\n",
      "print type(r14)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Row(_corrupt_record=None, actor=u'lastr2d2', actor_attributes=Row(blog=None, company=None, email=u'lastr2d2@gmail.com', gravatar_id=u'2a8a2ef556894cb1b6945a8c471bc4e9', location=None, login=u'lastr2d2', name=u'Wayne Wang', type=u'User'), created_at=u'2014-01-01T01:01:58-08:00', payload=Row(action=None, comment=None, comment_id=None, commit=None, desc=None, description=None, head=u'afa9b3ac304d6ab92fd7689d1604f240b8f4ae38', id=None, issue=None, issue_id=None, master_branch=None, member=None, membership_id=None, name=None, number=None, pages=None, pull_request=None, pusher_type=None, ref=u'refs/heads/master', ref_type=None, release=None, repository=None, shas=[[u'afa9b3ac304d6ab92fd7689d1604f240b8f4ae38', u'lastr2d2@gmail.com', u'updated minifized version', u'Wayne Wang', u'true']], size=1, team=None, url=None), public=True, repository=Row(archive_url=None, assignees_url=None, blobs_url=None, branches_url=None, clone_url=None, collaborators_url=None, comments_url=None, commits_url=None, compare_url=None, contents_url=None, contributors_url=None, created_at=u'2013-11-19T00:01:51-08:00', default_branch=None, description=u'My userscript for douban.fm', downloads_url=None, events_url=None, fork=False, forks=0, forks_count=None, forks_url=None, full_name=None, git_commits_url=None, git_refs_url=None, git_tags_url=None, git_url=None, has_downloads=True, has_issues=True, has_pages=None, has_wiki=True, homepage=None, hooks_url=None, html_url=None, id=14517966, integrate_branch=None, issue_comment_url=None, issue_events_url=None, issues_url=None, keys_url=None, labels_url=None, language=u'JavaScript', languages_url=None, master_branch=u'master', merges_url=None, milestones_url=None, mirror_url=None, name=u'scripts-doubanfm', notifications_url=None, open_issues=0, open_issues_count=None, organization=None, owner=u'lastr2d2', private=False, pulls_url=None, pushed_at=u'2014-01-01T01:01:57-08:00', releases_url=None, size=128, ssh_url=None, stargazers=0, stargazers_count=None, stargazers_url=None, statuses_url=None, subscribers_url=None, subscription_url=None, svn_url=None, tags_url=None, teams_url=None, trees_url=None, updated_at=None, url=u'https://github.com/lastr2d2/scripts-doubanfm', watchers=0, watchers_count=None), type=u'PushEvent', url=u'https://github.com/lastr2d2/scripts-doubanfm/compare/ba4d721b3d...afa9b3ac30')\n",
        "\n",
        "\n",
        "<class 'pyspark.sql.types.Row'>\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print r14.actor\n",
      "print r14.repository.name\n",
      "print r14.repository\n",
      "print r14.repository.owner\n",
      "print r14.repository.language"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TimothyGu\n",
        "mxe\n",
        "Row(archive_url=None, assignees_url=None, blobs_url=None, branches_url=None, clone_url=None, collaborators_url=None, comments_url=None, commits_url=None, compare_url=None, contents_url=None, contributors_url=None, created_at=u'2012-03-22T06:01:48-07:00', default_branch=None, description=u'MXE (M cross environment)', downloads_url=None, events_url=None, fork=False, forks=77, forks_count=None, forks_url=None, full_name=None, git_commits_url=None, git_refs_url=None, git_tags_url=None, git_url=None, has_downloads=True, has_issues=True, has_pages=None, has_wiki=False, homepage=u'http://mxe.cc', hooks_url=None, html_url=None, id=3797741, integrate_branch=None, issue_comment_url=None, issue_events_url=None, issues_url=None, keys_url=None, labels_url=None, language=u'C', languages_url=None, master_branch=u'master', merges_url=None, milestones_url=None, mirror_url=None, name=u'mxe', notifications_url=None, open_issues=2, open_issues_count=None, organization=u'mxe', owner=u'mxe', private=False, pulls_url=None, pushed_at=u'2013-12-28T17:14:53-08:00', releases_url=None, size=19727, ssh_url=None, stargazers=113, stargazers_count=None, stargazers_url=None, statuses_url=None, subscribers_url=None, subscription_url=None, svn_url=None, tags_url=None, teams_url=None, trees_url=None, updated_at=None, url=u'https://github.com/mxe/mxe', watchers=113, watchers_count=None)\n",
        "mxe\n",
        "C\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actor                created_at           id         org                  payload              public repo                 type                \n",
        "[https://avatars.... 2015-01-01T00:00:00Z 2489368070 null                 [null,86ffa724b4d... true   [28635890,davidjh... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:00Z 2489368072 null                 [null,d5a69a84e4d... true   [26392647,jmoon01... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:01Z 2489368089 null                 [null,null,null,n... true   [28677542,christo... CreateEvent         \n",
        "[https://avatars.... 2015-01-01T00:00:01Z 2489368095 [https://avatars.... [null,8590ddf9fe5... true   [5152285,square/o... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:03Z 2489368104 null                 [null,bc6e2f2a7d2... true   [28520835,git4rub... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:03Z 2489368109 null                 [null,2d7a0159d00... true   [25334511,tlgkcca... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:03Z 2489368114 null                 [null,277c2dca750... true   [21995378,Vilyan0... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:03Z 2489368113 null                 [null,305bfab9c61... true   [15526570,xndcn/d... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:03Z 2489368118 null                 [null,bda8f8fb466... true   [20268125,team3co... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:04Z 2489368129 null                 [null,null,null,n... true   [27156833,greyia/... CreateEvent         \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368130 null                 [null,863240e0b23... true   [3633660,piscisau... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368134 null                 [null,11f59532b73... true   [27858779,sean-sm... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368137 null                 [null,aedb53b2267... true   [20469468,caleb-e... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368138 null                 [null,38b7187cb1d... true   [24147122,sundaym... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368140 null                 [null,c77431db7e9... true   [28677492,joaoped... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:05Z 2489368145 null                 [null,d2de0eb5910... true   [20117626,ingydot... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:06Z 2489368151 [https://avatars.... [created,null,[nu... true   [27288715,uwarc/w... IssueCommentEvent   \n",
        "[https://avatars.... 2015-01-01T00:00:08Z 2489368159 null                 [null,42d4e629b4a... true   [28676854,vinniej... PushEvent           \n",
        "[https://avatars.... 2015-01-01T00:00:08Z 2489368161 [https://avatars.... [created,null,[[[... true   [28109544,selfhub... PullRequestReview...\n",
        "[https://avatars.... 2015-01-01T00:00:08Z 2489368162 null                 [null,88bdadfbd5d... true   [5089162,SCEDC/SC... PushEvent           \n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r15 = df15.first()\n",
      "print r15\n",
      "print \"\\n\"\n",
      "print type(r15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Row(actor=Row(avatar_url=u'https://avatars.githubusercontent.com/u/1310570?', gravatar_id=u'', id=1310570, login=u'soumith', url=u'https://api.github.com/users/soumith'), created_at=u'2015-01-01T01:00:00Z', id=u'2489395767', org=None, payload=Row(action=None, before=u'2d06657267b32e0c8e193c617039da200f710195', comment=None, commits=[Row(author=Row(email=u'soumith@gmail.com', name=u'Soumith Chintala'), distinct=True, message=u'back to old structure, except lua files moved out', sha=u'dbd68d30ee1f7b60d404553fc1c6226ebb374c8e', url=u'https://api.github.com/repos/soumith/fbcunn/commits/dbd68d30ee1f7b60d404553fc1c6226ebb374c8e'), Row(author=Row(email=u'soumith@gmail.com', name=u'Soumith Chintala'), distinct=True, message=u'...', sha=u'5567f9f5a83d7fe3320b18e5b89405e8a5ca77e6', url=u'https://api.github.com/repos/soumith/fbcunn/commits/5567f9f5a83d7fe3320b18e5b89405e8a5ca77e6'), Row(author=Row(email=u'soumith@gmail.com', name=u'Soumith Chintala'), distinct=True, message=u'...', sha=u'58a83b277328eca811d3a37cf171b2fc4fcd87af', url=u'https://api.github.com/repos/soumith/fbcunn/commits/58a83b277328eca811d3a37cf171b2fc4fcd87af'), Row(author=Row(email=u'soumith@gmail.com', name=u'Soumith Chintala'), distinct=True, message=u'...', sha=u'fa6048ec9b9eeafd12cee5f81324f355e1f2a198', url=u'https://api.github.com/repos/soumith/fbcunn/commits/fa6048ec9b9eeafd12cee5f81324f355e1f2a198')], description=None, distinct_size=4, forkee=None, head=u'fa6048ec9b9eeafd12cee5f81324f355e1f2a198', issue=None, master_branch=None, member=None, number=None, pages=None, pull_request=None, push_id=536752122, pusher_type=None, ref=u'refs/heads/master', ref_type=None, release=None, size=4), public=True, repo=Row(id=28067809, name=u'soumith/fbcunn', url=u'https://api.github.com/repos/soumith/fbcunn'), type=u'PushEvent')\n",
        "\n",
        "\n",
        "<class 'pyspark.sql.types.Row'>\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print r15.actor.login\n",
      "print r15.repo.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "soumith\n",
        "soumith/fbcunn\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df11_watch = df11.filter(\"type='WatchEvent'\")\n",
      "# df11_commit = df11.filter(\"type='CommitCommentEvent'\")\n",
      "# df11_fork = df11.filter(\"type='ForkEvent'\")\n",
      "# sqlContext.registerDataFrameAsTable(df11_watch, \"df11_watch_table\")\n",
      "# sqlContext.registerDataFrameAsTable(df11_commit, \"df11_commit_table\")\n",
      "# sqlContext.registerDataFrameAsTable(df11_fork, \"df11_fork_table\")\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch = df15.filter(\"type='WatchEvent'\")\n",
      "df15_commit = df15.filter(\"type='CommitCommentEvent'\")\n",
      "df15_fork = df15.filter(\"type='ForkEvent'\")\n",
      "\n",
      "sqlContext.registerDataFrameAsTable(df15_watch, \"df15_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_commit, \"df15_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_fork, \"df15_fork_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# df_union = sqlContext.sql(\"SELECT * from df_watch_table UNION ALL SELECT * from df_commit_table UNION ALL SELECT * from df_fork_table\")\n",
      "df15_union = sqlContext.sql(\"SELECT * from df15_watch_table UNION ALL SELECT * from df15_commit_table UNION ALL SELECT * from df15_fork_table\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_union.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "actor                created_at           id         org                  payload              public repo                 type      \n",
        "[https://avatars.... 2015-01-01T00:00:18Z 2489368319 [https://avatars.... [started,null,nul... true   [18297319,LinuxSt... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:24Z 2489368389 null                 [started,null,nul... true   [28229924,inf0rme... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:35Z 2489368485 null                 [started,null,nul... true   [28676020,parrt/c... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:42Z 2489368584 [https://avatars.... [started,null,nul... true   [26649719,Pathgat... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:50Z 2489368664 null                 [started,null,nul... true   [24036844,jtstern... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:57Z 2489368733 [https://avatars.... [started,null,nul... true   [6834703,Enterpri... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:00:59Z 2489368739 null                 [started,null,nul... true   [28660659,Victini... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:02Z 2489368777 null                 [started,null,nul... true   [4925652,robfig/c... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:27Z 2489368981 null                 [started,null,nul... true   [2992074,brendang... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:30Z 2489369013 [https://avatars.... [started,null,nul... true   [3402537,h5bp/Fro... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:43Z 2489369130 [https://avatars.... [started,null,nul... true   [20283101,pebble/... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:52Z 2489369213 [https://avatars.... [started,null,nul... true   [21853887,EU-OSHA... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:01:53Z 2489369223 null                 [started,null,nul... true   [11291794,daniels... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:02:00Z 2489369301 null                 [started,null,nul... true   [3502120,scottjeh... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:02:03Z 2489369327 [https://avatars.... [started,null,nul... true   [25449744,letsenc... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:02:55Z 2489369857 null                 [started,null,nul... true   [4878628,thlorenz... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:02:55Z 2489369862 [https://avatars.... [started,null,nul... true   [3390243,servo/se... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:02:55Z 2489369864 null                 [started,null,nul... true   [8650287,gaspaio/... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:03:07Z 2489369987 null                 [started,null,nul... true   [1445113,xpac27/L... WatchEvent\n",
        "[https://avatars.... 2015-01-01T00:03:24Z 2489370150 null                 [started,null,nul... true   [28625669,jpf/the... WatchEvent\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch = df15.filter(\"type='WatchEvent'\")\n",
      "df15_commit = df15.filter(\"type='CommitCommentEvent'\")\n",
      "df15_fork = df15.filter(\"type='ForkEvent'\")\n",
      "sqlContext.registerDataFrameAsTable(df15_watch, \"df15_watch_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_commit, \"df15_commit_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_fork, \"df15_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df15_watch_table\")\n",
      "df15_commit_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df15_commit_table\")\n",
      "df15_fork_altered = sqlContext.sql(\"SELECT actor, repo, type FROM df15_fork_table\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch_altered_rdd = df15_watch_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df15_commit_altered_rdd = df15_commit_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()\n",
      "df15_fork_altered_rdd = df15_fork_altered.map(lambda x: {\"actor\": x.actor.login, \"type\":x.type, \"repo\":x.repo.name}).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df15_watch_altered_df = sqlContext.createDataFrame(df15_watch_altered_rdd)\n",
      "df15_commit_altered_df = sqlContext.createDataFrame(df15_commit_altered_rdd)\n",
      "df15_fork_altered_df = sqlContext.createDataFrame(df15_fork_altered_rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Serialized task 16613:0 was 24230803 bytes, which exceeds max allowed: spark.akka.frameSize (10485760 bytes) - reserved (204800 bytes). Consider increasing spark.akka.frameSize or using broadcast variables for large values.\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-10-28bab81eb013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf15_watch_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf15_watch_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf15_commit_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf15_commit_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf15_fork_altered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf15_fork_altered_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \"\"\"\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, javaPartitions,\n\u001b[0;32m--> 842\u001b[0;31m                                           allowLocal)\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Serialized task 16613:0 was 24230803 bytes, which exceeds max allowed: spark.akka.frameSize (10485760 bytes) - reserved (204800 bytes). Consider increasing spark.akka.frameSize or using broadcast variables for large values.\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sqlContext.registerDataFrameAsTable(df15_watch_altered_df, \"df15_watch_altered_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_commit_altered_df, \"df15_commit_altered_table\")\n",
      "sqlContext.registerDataFrameAsTable(df15_fork_altered_df, \"df15_fork_altered_table\")\n",
      "\n",
      "df15_altered_union = sqlContext.sql(\"SELECT * from df15_watch_altered_table UNION ALL SELECT * from df15_commit_altered_table UNION ALL SELECT * from df15_fork_altered_table\")\n",
      "\n",
      "user_repo_map15 = df15_altered_union.map(lambda x: (x.actor, list([x.repo]))).groupByKey() \n",
      "user_repo15 = user_repo_map15.map(lambda x: {\"username\":x[0], \"repo\":[user for sublist in x[1] for user in sublist]}).collect()\n",
      "user_repo15_df = sqlContext.createDataFrame(user_repo15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_repo15[10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "{'repo': [u'benoitc/gunicorn', u'pietern/hiredis-py'],\n",
        " 'username': u'TimothyFitz'}"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_repo15_df.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "repo                 username       \n",
        "ArrayBuffer(angul... messa          \n",
        "ArrayBuffer(visio... spencermountain\n",
        "ArrayBuffer(code-... remram44       \n",
        "ArrayBuffer(jwand... in5ikt         \n",
        "ArrayBuffer(vjous... antonbabenko   \n",
        "ArrayBuffer(rails... null           \n",
        "ArrayBuffer(wavde... larsericsson   \n",
        "ArrayBuffer(olof/... profil         \n",
        "ArrayBuffer(route... PaulMarcel     \n",
        "ArrayBuffer(aloha... jacwright      \n",
        "ArrayBuffer(benoi... TimothyFitz    \n",
        "ArrayBuffer(zendf... vchabot        \n",
        "ArrayBuffer(docum... medihack       \n",
        "ArrayBuffer(awass... lovoror        \n",
        "ArrayBuffer(bobth... PeterGumball   \n",
        "ArrayBuffer(utaht... kazutoyo       \n",
        "ArrayBuffer(Pasca... sintaxasn      \n",
        "ArrayBuffer(jquer... isaias         \n",
        "ArrayBuffer(Snorb... binaryone      \n",
        "ArrayBuffer(haxza... xiuide         \n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = df_union.first()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "Row(_corrupt_record=None, actor=u'{\"gravatar_id\":\"cd73497eb3c985f302723424c3fa5b50\",\"url\":\"https://api.github.dev/users/sosedoff\",\"id\":71051,\"login\":\"sosedoff\",\"avatar_url\":\"https://secure.gravatar.com/avatar/cd73497eb3c985f302723424c3fa5b50?d=http://github.dev%2Fimages%2Fgravatars%2Fgravatar-user-420.png\"}', actor_attributes=None, created_at=u'2011-02-12T00:00:06Z', id=u'1127195541', org=None, payload=Row(action=u'started', actor=u'sosedoff', actor_gravatar=u'cd73497eb3c985f302723424c3fa5b50', after=None, before=None, comment=None, comment_id=None, commit=None, commits=None, desc=None, description=None, download=None, forkee=None, gist=None, head=None, id=None, issue=None, issue_id=None, legacy=None, master_branch=None, member=None, membership_id=None, name=None, number=None, object=None, object_name=None, original=None, page_name=None, pages=None, pull_request=None, push_id=None, pusher_type=None, ref=None, ref_type=None, release=None, repo=u'ezmobius/super-nginx', repository=None, sha=None, shas=None, size=None, snippet=None, summary=None, target=None, team=None, title=None, url=None), public=u'true', repo=Row(id=1357116, name=u'ezmobius/super-nginx', url=u'https://api.github.dev/repos/ezmobius/super-nginx'), repository=None, type=u'WatchEvent', url=None)"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ast.literal_eval\n",
      "\n",
      "print r.actor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"gravatar_id\":\"cd73497eb3c985f302723424c3fa5b50\",\"url\":\"https://api.github.dev/users/sosedoff\",\"id\":71051,\"login\":\"sosedoff\",\"avatar_url\":\"https://secure.gravatar.com/avatar/cd73497eb3c985f302723424c3fa5b50?d=http://github.dev%2Fimages%2Fgravatars%2Fgravatar-user-420.png\"}\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "u'platform_frameworks_support'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.repository"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "Row(created_at=u'2012-04-09T21:06:44-07:00', description=u'', fork=False, forks=81, has_downloads=True, has_issues=False, has_wiki=True, homepage=u'', id=3978781, integrate_branch=None, language=u'Java', master_branch=u'master', name=u'platform_frameworks_support', open_issues=0, organization=u'android', owner=u'android', private=False, pushed_at=u'2013-12-11T12:50:30-08:00', size=8279, stargazers=118, url=u'https://github.com/android/platform_frameworks_support', watchers=118)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.repository.owner + '/' + r.repository.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "u'android/platform_frameworks_support'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "u'ForkEvent'"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}